{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Customization & Prompt Engineering in AutoGen v0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "model_client = OpenAIChatCompletionClient(model='gpt-4', api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Customization\n",
    "\n",
    "- We can assign a role to our agent\n",
    "- help in fitting agent to specific use case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "asssistant = AssistantAgent(\n",
    "    name = 'history_expert',\n",
    "    model_client=model_client,\n",
    "    description='A knowledgeable assistant with expertise in world history',\n",
    "    system_message='You are a history expert with deep knowledge of world history. Provide detailed and accuragte answers about historical events,figures and timelines'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_history_expert():\n",
    "    result = await asssistant.run(task = 'Who was the first President of USA?')\n",
    "    print(result.messages[-1].content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'The model `gpt-4` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m test_history_expert()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mtest_history_expert\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtest_history_expert\u001b[39m():\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m asssistant.run(task = \u001b[33m'\u001b[39m\u001b[33mWho was the first President of USA?\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(result.messages[-\u001b[32m1\u001b[39m].content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\PROJECTS\\AutoGen\\venv\\Lib\\site-packages\\autogen_agentchat\\agents\\_base_chat_agent.py:149\u001b[39m, in \u001b[36mBaseChatAgent.run\u001b[39m\u001b[34m(self, task, cancellation_token, output_task_messages)\u001b[39m\n\u001b[32m    147\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    148\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid message type in sequence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(msg)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.on_messages(input_messages, cancellation_token)\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.inner_messages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    151\u001b[39m     output_messages += response.inner_messages\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\PROJECTS\\AutoGen\\venv\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py:896\u001b[39m, in \u001b[36mAssistantAgent.on_messages\u001b[39m\u001b[34m(self, messages, cancellation_token)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mon_messages\u001b[39m(\n\u001b[32m    883\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    884\u001b[39m     messages: Sequence[BaseChatMessage],\n\u001b[32m    885\u001b[39m     cancellation_token: CancellationToken,\n\u001b[32m    886\u001b[39m ) -> Response:\n\u001b[32m    887\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Process incoming messages and generate a response.\u001b[39;00m\n\u001b[32m    888\u001b[39m \n\u001b[32m    889\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    894\u001b[39m \u001b[33;03m        Response containing the agent's reply\u001b[39;00m\n\u001b[32m    895\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m896\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.on_messages_stream(messages, cancellation_token):\n\u001b[32m    897\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, Response):\n\u001b[32m    898\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m message\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\PROJECTS\\AutoGen\\venv\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py:953\u001b[39m, in \u001b[36mAssistantAgent.on_messages_stream\u001b[39m\u001b[34m(self, messages, cancellation_token)\u001b[39m\n\u001b[32m    951\u001b[39m \u001b[38;5;66;03m# STEP 4: Run the first inference\u001b[39;00m\n\u001b[32m    952\u001b[39m model_result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m953\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m inference_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_llm(\n\u001b[32m    954\u001b[39m     model_client=model_client,\n\u001b[32m    955\u001b[39m     model_client_stream=model_client_stream,\n\u001b[32m    956\u001b[39m     system_messages=system_messages,\n\u001b[32m    957\u001b[39m     model_context=model_context,\n\u001b[32m    958\u001b[39m     workbench=workbench,\n\u001b[32m    959\u001b[39m     handoff_tools=handoff_tools,\n\u001b[32m    960\u001b[39m     agent_name=agent_name,\n\u001b[32m    961\u001b[39m     cancellation_token=cancellation_token,\n\u001b[32m    962\u001b[39m     output_content_type=output_content_type,\n\u001b[32m    963\u001b[39m     message_id=message_id,\n\u001b[32m    964\u001b[39m ):\n\u001b[32m    965\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inference_output, CreateResult):\n\u001b[32m    966\u001b[39m         model_result = inference_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\PROJECTS\\AutoGen\\venv\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py:1107\u001b[39m, in \u001b[36mAssistantAgent._call_llm\u001b[39m\u001b[34m(cls, model_client, model_client_stream, system_messages, model_context, workbench, handoff_tools, agent_name, cancellation_token, output_content_type, message_id)\u001b[39m\n\u001b[32m   1105\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m model_result\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1107\u001b[39m     model_result = \u001b[38;5;28;01mawait\u001b[39;00m model_client.create(\n\u001b[32m   1108\u001b[39m         llm_messages,\n\u001b[32m   1109\u001b[39m         tools=tools,\n\u001b[32m   1110\u001b[39m         cancellation_token=cancellation_token,\n\u001b[32m   1111\u001b[39m         json_output=output_content_type,\n\u001b[32m   1112\u001b[39m     )\n\u001b[32m   1113\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m model_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\PROJECTS\\AutoGen\\venv\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py:691\u001b[39m, in \u001b[36mBaseOpenAIChatCompletionClient.create\u001b[39m\u001b[34m(self, messages, tools, tool_choice, json_output, extra_create_args, cancellation_token)\u001b[39m\n\u001b[32m    689\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    690\u001b[39m     cancellation_token.link_future(future)\n\u001b[32m--> \u001b[39m\u001b[32m691\u001b[39m result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[32m    692\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m create_params.response_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    693\u001b[39m     result = cast(ParsedChatCompletion[Any], result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\PROJECTS\\AutoGen\\venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2583\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2537\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   2538\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   2539\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2580\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   2581\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m   2582\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2583\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2584\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2585\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2586\u001b[39m             {\n\u001b[32m   2587\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2588\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2589\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2590\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2591\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2592\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2593\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2594\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2595\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2596\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2597\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2598\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2599\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2600\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2601\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2602\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2603\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   2604\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2605\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2606\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   2607\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2608\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2609\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2610\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2611\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2612\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2613\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2614\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2615\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2616\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2617\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2618\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2619\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: verbosity,\n\u001b[32m   2620\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2621\u001b[39m             },\n\u001b[32m   2622\u001b[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2623\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2624\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2625\u001b[39m         ),\n\u001b[32m   2626\u001b[39m         options=make_request_options(\n\u001b[32m   2627\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2628\u001b[39m         ),\n\u001b[32m   2629\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   2630\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2631\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2632\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\PROJECTS\\AutoGen\\venv\\Lib\\site-packages\\openai\\_base_client.py:1794\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1780\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1781\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1782\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1789\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1790\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1791\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1792\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1793\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\PROJECTS\\AutoGen\\venv\\Lib\\site-packages\\openai\\_base_client.py:1594\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1591\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1593\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1594\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1596\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1598\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNotFoundError\u001b[39m: Error code: 404 - {'error': {'message': 'The model `gpt-4` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}"
     ]
    }
   ],
   "source": [
    "await test_history_expert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first President of the United States was George Washington. He served from 1789 to 1797. Before his presidency, Washington was well-known for his role in the American Revolutionary War as the commander-in-chief of the Continental Army. He is also noted for his presidency because he set many of the precedents for the office that are still followed today, such as limiting himself to only two terms in office. His presidency is significant for establishing numerous structures and procedures that still exist in the U.S. government today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering\n",
    "\n",
    "[Prompt Engineering - OpenAI ](https://platform.openai.com/docs/guides/prompt-engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asssistant = AssistantAgent(\n",
    "    name = 'history_expert',\n",
    "    model_client=model_client,\n",
    "    description='A knowledgeable assistant with expertise in world history',\n",
    "    system_message='You are a history expert with deep knowledge of world history who explaines event in a storytelling style, as if narrating a historic novel. Be vivid and engaging and focus on figures and events'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_storytelling():\n",
    "    result = await asssistant.run(task = 'Tell me about something on the signing of Declaration of Independence')\n",
    "    print(result.messages[-1].content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter One: The Thirteen Colonies, July 4, 1776\n",
      "\n",
      "Like a sulking storm, tension had been brewing for months in the thirteen American colonies. For too long, they had been the marionettes dancing to the discordant tunes of a British puppeteer. The trampling of their rights was accompanied by the haughty laughter of an invincible monarch oceans away — King George III. But the people of the colonies were not to be owned. They craved freedom and stoked a rebellion that echoed through the chambers of history.\n",
      "\n",
      "Chapter Two: The Secret Chamber, June 7, 1776\n",
      "\n",
      "The smoke of discontent rose from the cobblestone streets and found its way into the hallowed sanctuary of the Continental Congress in Philadelphia. It was inside this aflame room, dripping with anticipation, that Richard Henry Lee of Virginia posed the challenge of a lifetime. He proposed a resolution and those revolutionary words reverberated through the air, \"Resolved: That these United Colonies are, and of right ought to be, free and independent States.\"\n",
      "\n",
      "Chapter Three: The Committee of Five, June 11, 1776\n",
      "\n",
      "Five men were chosen to draft an audacious document, a brazen clarion call to the world. These were no ordinary men, but visionaries. You had John Adams of Massachusetts, Benjamin Franklin of Pennsylvania, Thomas Jefferson of Virginia, Robert R. Livingston of New York and Roger Sherman of Connecticut. They burned the midnight oil, their quills dancing furiously across parchments, the light of the solitary candle flickering in the eyes of their determination.\n",
      "\n",
      "Chapter Four: The Avid Artisan, July 2, 1776\n",
      "\n",
      "Fair and frail, a parchment lay on the table. Upon it, the euphoria, dreams, rebels, and freedom of a burgeoning nation. Etched in the eloquence of Thomas Jefferson’s words was a beacon - a Declaration of Independence. A portrait of courage and resilience, this document voiced the silent cries of the oppressed and struck the chords of liberty.\n",
      "\n",
      "Chapter Five: The Unanimous Declaration, July 4, 1776\n",
      "\n",
      "The hot summer day of July 4th was about to etch an indelible mark on history’s sprawling canvas. With a mix of apprehension and excitement in the air, the elegant prose of the Declaration was read aloud to the members. With each word, their resolve only hardened. They knew the stakes, life or death. The bell rang loud, summoning the members to inscribe their approval.\n",
      "\n",
      "Chapter Six: The Sign, August 2, 1776\n",
      "\n",
      "In a ceremony marked by equal parts solemnity and celebration, 56 delegates ritually signed the handwritten parchment. The bold stroke of John Hancock, the President of the Congress, was a testament to their defiance. As each signature furrowed into the parchment, a new nail hammered into the coffin of British rule. With each quill lowered, a brave new world of American liberty was conceived.\n",
      "\n",
      "Epilogue: The Liberty Bell, July 8, 1776\n",
      "\n",
      "Four days later, a deeply resonant bell echoed through the streets of Philadelphia. The Liberty Bell, it was called. As its chimes topped the air, the citizens gathered to listen to the first public reading of the Declaration of Independence. The bell's loud bronze voice was a symbol of their newfound freedom, a song of revolution, triumph, and the extraordinary birth of a nation.\n"
     ]
    }
   ],
   "source": [
    "await test_storytelling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in New York is sunny!\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load API key from .env\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model_client = OpenAIChatCompletionClient(model=\"gpt-4\", api_key=api_key)\n",
    "\n",
    "# Define a simple tool\n",
    "def get_weather(city: str) -> str:\n",
    "    return f\"The weather in {city} is sunny!\"  # Dummy response\n",
    "\n",
    "# Set up the agent\n",
    "agent = AssistantAgent(\n",
    "    name=\"weather_agent\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"You are a weather assistant. Use the get_weather tool when asked about weather.\",\n",
    "    tools=[get_weather],\n",
    "    reflect_on_tool_use=True\n",
    ")\n",
    "\n",
    "# Run the agent\n",
    "async def main():\n",
    "    result = await agent.run(task=\"What’s the weather in New York?\")\n",
    "    print(result.messages[-1].content)\n",
    "\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
